# Анализ использования потоков и параллелизма

## Текущая конфигурация

- **MAX_WORKERS**: 4 (по умолчанию)
- **CPU в контейнере**: До 8 ядер (без ограничений в docker-compose)

## Структура потоков (goroutines)

### 1. Event Log Readers (Журнал регистрации)

**Уровень 1: Reader'ы по базам данных**
- По одной горутине на каждую найденную базу данных (location)
- Количество зависит от количества баз в `LOG_DIRS`

**Уровень 2: Worker pool внутри каждого reader'а**
- `maxWorkers` горутин для параллельной обработки `.lgp` файлов
- По умолчанию: **4 горутины**
- Каждая горутина обрабатывает один файл за раз

**Уровень 3: Вспомогательные горутины**
- 1 горутина для подсчета записей (`counting goroutine`)
- 1 горутина для стриминга файлов (`streamFiles`)

**Пример расчета:**
- Если найдено 2 базы данных:
  - 2 reader'а (верхний уровень)
  - 2 × 4 = 8 воркеров для обработки файлов
  - 2 × 1 = 2 горутины для подсчета
  - 1 горутина для стриминга (общая)
  - **Итого: ~13 горутин для event log**

### 2. Tech Log Tailers (Технологический журнал)

**Уровень 1: Tailer'ы по каталогам**
- По одной горутине на каждый каталог в `TECHLOG_DIRS`
- Обычно: 1 каталог = 1 горутина

**Уровень 2: Worker pool внутри каждого tailer'а**
- `maxWorkers` горутин для параллельной обработки исторических файлов
- По умолчанию: **4 горутины**
- Каждая горутина обрабатывает один файл за раз

**Уровень 3: Вспомогательные горутины**
- 1 горутина для запуска tailer'а (`go func()` в parser_service.go:572)

**Пример расчета:**
- Если 1 каталог tech log:
  - 1 tailer (верхний уровень)
  - 1 × 4 = 4 воркера для обработки файлов
  - 1 горутина для запуска
  - **Итого: ~6 горутин для tech log**

### 3. New Errors Worker

- 1 горутина для периодической агрегации ошибок
- Запускается каждые `NEW_ERRORS_UPDATE_INTERVAL` минут (по умолчанию: 10)

### 4. ClickHouse Writer

- **Синхронные операции**: Flush выполняется синхронно в горутине, которая вызывает `WriteEventLog`/`WriteTechLog`
- **Нет отдельного пула потоков**: Все операции записи выполняются в горутинах reader'ов/tailer'ов
- **Mutex защита**: `batchMutex` защищает доступ к батчам

## Общее количество горутин

**Типичный сценарий (2 базы данных, 1 каталог tech log):**
```
Event Log:
  - 2 reader'а (верхний уровень)
  - 2 × 4 = 8 воркеров для файлов
  - 2 × 1 = 2 горутины для подсчета
  - 1 горутина для стриминга
  = 13 горутин

Tech Log:
  - 1 tailer (верхний уровень)
  - 1 × 4 = 4 воркера для файлов
  - 1 горутина для запуска
  = 6 горутин

New Errors Worker:
  - 1 горутина
  = 1 горутина

ИТОГО: ~20 горутин
```

## Использование CPU

### Почему используется до 8 ядер?

1. **Go runtime планировщик**: По умолчанию использует все доступные CPU ядра (`GOMAXPROCS = количество ядер`)
2. **Параллельная обработка файлов**: 
   - Event log: до 4 файлов одновременно на базу данных
   - Tech log: до 4 файлов одновременно
   - Если несколько баз: умножение на количество баз
3. **Параллельные операции I/O**:
   - Чтение файлов
   - Парсинг данных
   - Запись в ClickHouse (может быть параллельной от разных reader'ов)

### Ограничения

- **Нет ограничений CPU в docker-compose**: Контейнер может использовать все доступные ядра хоста
- **MAX_WORKERS=4**: Ограничивает параллелизм внутри каждого reader'а/tailer'а, но не общий параллелизм

## Рекомендации по оптимизации

### Если CPU перегружен:

1. **Уменьшить MAX_WORKERS**:
   ```bash
   MAX_WORKERS=2
   ```
   - Снизит параллелизм обработки файлов
   - Уменьшит нагрузку на CPU
   - Увеличит время обработки

2. **Ограничить CPU в docker-compose**:
   ```yaml
   log-parser:
     deploy:
       resources:
         limits:
           cpus: '4'
   ```
   - Ограничит использование CPU контейнером
   - Docker будет ограничивать планировщик

3. **Увеличить BATCH_SIZE**:
   ```bash
   BATCH_SIZE=10000
   ```
   - Уменьшит количество операций записи в ClickHouse
   - Снизит нагрузку на I/O

### Если CPU недогружен:

1. **Увеличить MAX_WORKERS**:
   ```bash
   MAX_WORKERS=8
   ```
   - Увеличит параллелизм обработки файлов
   - Ускорит обработку при наличии свободных CPU

2. **Убрать ограничения CPU** (если были установлены)

## Мониторинг

### Проверка текущего использования:

```bash
# CPU и память контейнера
docker stats 1c-log-parser --no-stream

# Количество горутин (требует доступа к runtime)
# Можно добавить метрики в код через runtime.NumGoroutine()
```

### Логи для анализа:

- Количество найденных баз данных: `Found event log locations`
- Количество воркеров: `max_workers` в логах reader'ов/tailer'ов
- Время обработки файлов: метрики в `parser_metrics`

## Выводы

1. **Количество потоков зависит от**:
   - Количества баз данных (event log)
   - Количества каталогов tech log
   - Значения `MAX_WORKERS`
   - Go runtime использует все доступные CPU ядра

2. **8 ядер используются потому что**:
   - Go runtime планировщик распределяет горутины по всем доступным ядрам
   - Параллельная обработка файлов от разных reader'ов
   - Нет ограничений CPU в docker-compose

3. **Для контроля нагрузки**:
   - Настроить `MAX_WORKERS` в зависимости от количества CPU
   - При необходимости добавить ограничения CPU в docker-compose
   - Мониторить метрики производительности

